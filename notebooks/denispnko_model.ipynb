{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5c4c33e-5494-4769-941f-e2a4a4a3120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "sys.path.append('../')\n",
    "from vocal_patterns.ml_logic.preprocessor import preprocess_audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3237608-9751-4da1-b79a-4bb80beb216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model functions\n",
    "\n",
    "def initialize_model(input_shape) -> Model:\n",
    "    \"\"\"\n",
    "    Initialize the CNN model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(8, (5,5), input_shape=input_shape, strides=(2, 2), padding='same', activation=\"relu\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Conv2D(16, (3,3), strides=(2, 2), padding='same', activation=\"relu\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Conv2D(32, (3,3), strides=(2, 2), padding='same', activation=\"relu\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3,3), strides=(2, 2), padding='same', activation=\"relu\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    print(\"✅ Model initialized\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def compile_model(model: Model, learning_rate=0.001) -> Model:\n",
    "    \"\"\"\n",
    "    Compile the Neural Network\n",
    "    \"\"\"\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    print(\"✅ Model compiled\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e1b1e7c-048a-4ea6-80d9-725555f566d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>exercise</th>\n",
       "      <th>technique</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/denis/code/ElsaGregoire/vocal_patterns/...</td>\n",
       "      <td>Other</td>\n",
       "      <td>vibrato</td>\n",
       "      <td>m6_row_vibrato.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/denis/code/ElsaGregoire/vocal_patterns/...</td>\n",
       "      <td>Other</td>\n",
       "      <td>vibrato</td>\n",
       "      <td>m6_caro_vibrato.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/denis/code/ElsaGregoire/vocal_patterns/...</td>\n",
       "      <td>Other</td>\n",
       "      <td>vibrato</td>\n",
       "      <td>m6_dona_vibrato.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/denis/code/ElsaGregoire/vocal_patterns/...</td>\n",
       "      <td>Other</td>\n",
       "      <td>straight</td>\n",
       "      <td>m6_caro_straight.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/denis/code/ElsaGregoire/vocal_patterns/...</td>\n",
       "      <td>Other</td>\n",
       "      <td>straight</td>\n",
       "      <td>m6_row_straight.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path exercise technique  \\\n",
       "0  /Users/denis/code/ElsaGregoire/vocal_patterns/...    Other   vibrato   \n",
       "1  /Users/denis/code/ElsaGregoire/vocal_patterns/...    Other   vibrato   \n",
       "2  /Users/denis/code/ElsaGregoire/vocal_patterns/...    Other   vibrato   \n",
       "3  /Users/denis/code/ElsaGregoire/vocal_patterns/...    Other  straight   \n",
       "4  /Users/denis/code/ElsaGregoire/vocal_patterns/...    Other  straight   \n",
       "\n",
       "               filename  \n",
       "0    m6_row_vibrato.wav  \n",
       "1   m6_caro_vibrato.wav  \n",
       "2   m6_dona_vibrato.wav  \n",
       "3  m6_caro_straight.wav  \n",
       "4   m6_row_straight.wav  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the data\n",
    "\n",
    "download_path = \"../vocal_patterns/data/dataset_tags.csv\"\n",
    "data = pd.read_csv(download_path)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate the data\n",
    "\n",
    "X = data.drop(columns=['exercise', \"technique\", \"filename\"])\n",
    "y = data[['exercise']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = preprocess_audio(X_train)\n",
    "X_train_reshaped = X_train_processed.reshape(len(X_train_processed), 128, 259, 1)\n",
    "\n",
    "X_test_processed = preprocess_audio(X_test)\n",
    "X_test_reshaped = X_test_processed.reshape(len(X_test_processed), 128, 259, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/denis/.pyenv/versions/3.10.6/envs/vocal_patterns/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train_labels = label_encoder.fit_transform(y_train)\n",
    "y_train_cat = to_categorical(y_train_labels, num_classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/denis/.pyenv/versions/3.10.6/envs/vocal_patterns/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_test_labels = label_encoder.fit_transform(y_test)\n",
    "y_test_cat = to_categorical(y_test_labels, num_classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model initialized\n",
      "✅ Model compiled\n"
     ]
    }
   ],
   "source": [
    "model = initialize_model((128, 259, 1))\n",
    "model = compile_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 [==============================] - 3s 40ms/step - loss: 0.5228 - accuracy: 0.7870 - val_loss: 1.0709 - val_accuracy: 0.3221\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 2s 37ms/step - loss: 0.0920 - accuracy: 0.9644 - val_loss: 0.9456 - val_accuracy: 0.5217\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 2s 38ms/step - loss: 0.0147 - accuracy: 0.9965 - val_loss: 0.8256 - val_accuracy: 0.6028\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 2s 39ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.7429 - val_accuracy: 0.6443\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 3s 39ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.4397 - val_accuracy: 0.8379\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 2s 39ms/step - loss: 9.2393e-04 - accuracy: 1.0000 - val_loss: 0.3041 - val_accuracy: 0.8893\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 2s 38ms/step - loss: 6.8480e-04 - accuracy: 1.0000 - val_loss: 0.2186 - val_accuracy: 0.9249\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 2s 38ms/step - loss: 5.9496e-04 - accuracy: 1.0000 - val_loss: 0.1712 - val_accuracy: 0.9368\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 2s 39ms/step - loss: 4.2981e-04 - accuracy: 1.0000 - val_loss: 0.1565 - val_accuracy: 0.9447\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 3s 40ms/step - loss: 3.2362e-04 - accuracy: 1.0000 - val_loss: 0.1528 - val_accuracy: 0.9486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x296c68be0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_reshaped, y_train_cat, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 11ms/step - loss: 0.1358 - accuracy: 0.9539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1357562392950058, 0.9538745284080505]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_reshaped, y_test_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
